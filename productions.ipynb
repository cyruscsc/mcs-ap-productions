{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production 1: Data translation for storage\n",
    "\n",
    "To enable a program to function each time it runs there needs to be an external and persistent data storage system that retains the state of the data. There are two considerations here. The physical storage medium for the data, such as a file or database and the format/struture of the data. This week you have explored a number of different formats in both regards as follows:\n",
    "\n",
    "- CSV\n",
    "- XML\n",
    "- JSON\n",
    "- MongoDB\n",
    "- SQL Database\n",
    "\n",
    "Select ONE format that you consider as most suited to the data in the scenario and the aims of the program (client brief/or your own data). The format selected should support both the nature of the data and the aims of the application being designed. It should provide distinct advantages and minimal limitations over other data formats. It should not be selected solely because it is the easiest to program, although this can be included as an advantage if applicable.\n",
    "\n",
    "### Design\n",
    "\n",
    "Produce a model that shows how the data needs to be restructured to take best advantage of the selected format and work more effectively within the program. Where you have created groups or objects from the data show how they relate to each other.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Implement a parser that reads in the original data file. You may want to create a subset of the data file for testing and speed. Your program should then perform the translation from the original format/structure into your selected format. The result of this process should then be outputted to its relevant physical medium (files/database).\n",
    "\n",
    "At this stage there is no requirement to handle data types (other than those inherent in the data format, i.e. numbers and “Strings”), conversions or missing data. The program can be demonstrated as a simple console based application, requiring the input of the file name by the user and sufficient output to demonstrate the correctness of the translation process.\n",
    "\n",
    "Your program should produce regular output statements to the console so that it is easy to follow what the program is doing and provides a visual demonstration of the translation process. This will also eb handy for any debugging required.\n",
    "\n",
    "### Reflection on design decisions\n",
    "\n",
    "Write a 200-word reflection that states the reason for your format selection and the advantages the format leads to the data and application and any limitations on the future use of this data within the selected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = 'datasets/'\n",
    "OUT_DIR = 'outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV parser\n",
    "\n",
    "\n",
    "class CSV:\n",
    "    def __init__(self, csv_file):\n",
    "        self.__file = csv_file\n",
    "        self.__headers = []\n",
    "        self.__data = []\n",
    "        self.__parse()\n",
    "\n",
    "    def __readln(self, line):\n",
    "        return line.strip().split(\",\")\n",
    "\n",
    "    def __parse(self):\n",
    "        with open(self.__file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if not self.__headers:\n",
    "                    self.__headers = self.__readln(line)\n",
    "                else:\n",
    "                    self.__data.append(self.__readln(line))\n",
    "\n",
    "    @property\n",
    "    def headers(self):\n",
    "        return self.__headers\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.__data\n",
    "\n",
    "    def to_dict(self):\n",
    "        return [\n",
    "            {self.__headers[i]: row[i] for i in range(len(self.__headers))}\n",
    "            for row in self.__data\n",
    "        ]\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main functions\n",
    "\n",
    "\n",
    "def parse_csv():\n",
    "    csv_filename = input(\"Enter the csv filename: \")\n",
    "    csv_filepath = SRC_DIR + csv_filename\n",
    "    if not os.path.exists(csv_filepath):\n",
    "        print(f\"File {csv_filepath} not found\")\n",
    "        return None\n",
    "    print(f\"Parsing CSV file {csv_filename}...\")\n",
    "    csv_parser = CSV(csv_filepath)\n",
    "    print(f\"CSV file {csv_filename} parsed successfully\")\n",
    "    return csv_parser\n",
    "\n",
    "\n",
    "def csv_to_json(csv_parser):\n",
    "    json_filename = input(\"Enter the json filename: \")\n",
    "    json_filepath = OUT_DIR + json_filename\n",
    "    print(f\"Exporting JSON file {json_filename}...\")\n",
    "    os.makedirs(os.path.dirname(json_filepath), exist_ok=True)\n",
    "    with open(json_filepath, \"w\") as f:\n",
    "        f.write(csv_parser.to_json())\n",
    "    print(f\"JSON file {json_filename} exported successfully\")\n",
    "\n",
    "\n",
    "def translate_data():\n",
    "    csv_parser = parse_csv()\n",
    "    if csv_parser:\n",
    "        print()\n",
    "        csv_to_json(csv_parser)\n",
    "    else:\n",
    "        print(\"Exited due to error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production 3: Data Cleaning and Initial Analysis\n",
    "\n",
    "Given the client brief, there are a number of requirements to provide accesses to specific parts of the data and provide answers to specific statistical questions. For this production focus on how your application will manipulate the data (cleaning and shaping) and developing functions for calculating some of the statically requirements.\n",
    "\n",
    "### Design\n",
    "\n",
    "Consider the steps required for cleaning and shaping and any of the calculations (functions) you want to develop. Write pseudocode to sketch these out before you write code. Mentally or on paper walk the data through your pseudocode steps to test how effective your solution is.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The first stage is to clean the data and make sure it is fit for purpose. Examine the data careful to identify anomalies and then consider how your program can identify these and correct/delete or change. You will need to consider how you are going to handle erroneous or missing values. You should output a sample of the data that demonstrates how cleaning has changed the data.\n",
    "\n",
    "The next stage is to reshape the data as per any requirements of the brief (or your own scenario). Is all the data needed to provide the required results? Is any of it duplicated? Is there data across different sources that needs to be brought together? Again, output a sample to the console to demonstrate how this has changed the structure of the data.\n",
    "\n",
    "Finally develop and test a set of functions (or objects and methods) that applies the statistical analysis to the data set, outputting the results to the console.\n",
    "\n",
    "Capture the results of your data cleaning, shaping and functions with screenshots of the consol. There is no requirement at this stage for anything to be functioning in through the GUI. Make sure it is clear what your output is testing/demonstrating (output simple informative statements).\n",
    "\n",
    "### Reflection on design decisions\n",
    "\n",
    "Write a 200-word reflection that states why you have selected specific tools from NumPy or pandas. This may be the data structure you have used, the functions you have applied to clean and shape the data. Clearly identify which specific aspects of the requirements or data’s structure informed the decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF:\n",
    "    def __init__(self, csv_file):\n",
    "        self.__df = pd.read_csv(csv_file)\n",
    "\n",
    "    @property\n",
    "    def columns(self) -> pd.Index:\n",
    "        return self.__df.columns\n",
    "\n",
    "    @property\n",
    "    def data(self) -> pd.DataFrame:\n",
    "        return self.__df\n",
    "\n",
    "    def clean(self) -> \"DF\":\n",
    "        \"\"\"Remove rows with missing values and duplicates\"\"\"\n",
    "        self.__df.dropna(inplace=True)\n",
    "        self.__df.drop_duplicates(inplace=True)\n",
    "        return self\n",
    "\n",
    "    def remove(self, column: str, value: object) -> \"DF\":\n",
    "        \"\"\"Remove rows where column is equal to value\"\"\"\n",
    "        self.__df = self.__df[self.__df[column] != value]\n",
    "        return self\n",
    "\n",
    "    def rename(self, column: str, new_name: str) -> \"DF\":\n",
    "        \"\"\"Rename a column\"\"\"\n",
    "        self.__df.rename(columns={column: new_name}, inplace=True)\n",
    "        return self\n",
    "\n",
    "    def merge(self, df: \"DF\", on: str, how: str = \"inner\") -> \"DF\":\n",
    "        \"\"\"Merge two dataframes\"\"\"\n",
    "        self.__df = self.__df.merge(df.data, on=on, how=how)\n",
    "        return self\n",
    "\n",
    "    def to_json(self, filename: str):\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        self.__df.to_json(filename, orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main functions\n",
    "\n",
    "\n",
    "def load_dfs():\n",
    "    df_dict = {}\n",
    "    while True:\n",
    "        csv_filename = input(\"Enter the csv filename (or leave empty to stop): \")\n",
    "        if not csv_filename:\n",
    "            break\n",
    "        csv_filepath = SRC_DIR + csv_filename\n",
    "        if not os.path.exists(csv_filepath):\n",
    "            print(f\"File {csv_filepath} not found\")\n",
    "            continue\n",
    "        df_name = input(\"Enter the dataframe name: \")\n",
    "        print(f\"Loading CSV file {csv_filename}...\")\n",
    "        df_dict[df_name] = DF(csv_filepath)\n",
    "        print(f\"CSV file {csv_filename} loaded to dataframe {df_name} successfully\")\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def clean_dfs(df_dict):\n",
    "    for name, df in df_dict.items():\n",
    "        print(f\"Cleaning dataframe {name}...\")\n",
    "        df.clean()\n",
    "        print(f\"Dataframe {name} cleaned successfully\")\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def remove_rows(df_dict):\n",
    "    while True:\n",
    "        print(\"Select a dataframe to remove rows from:\")\n",
    "        for i, name in enumerate(df_dict.keys()):\n",
    "            print(f\"{i+1}. {name}\")\n",
    "        df_num = input(\"Enter the dataframe index (or leave empty to stop): \")\n",
    "        if not df_num:\n",
    "            break\n",
    "        try:\n",
    "            df_num = int(df_num)\n",
    "            if df_num < 1 or df_num > len(df_dict):\n",
    "                raise ValueError\n",
    "        except ValueError:\n",
    "            print(\"Invalid index\")\n",
    "            continue\n",
    "        df_name = list(df_dict.keys())[df_num - 1]\n",
    "        column = input(\"Enter the column name: \")\n",
    "        value = input(\"Enter the value to remove: \")\n",
    "        print(f\"Removing rows where {column} is {value} from dataframe {df_name}...\")\n",
    "        df_dict[df_name].remove(column, value)\n",
    "        print(f\"Rows removed successfully\")\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def rename_columns(df_dict):\n",
    "    while True:\n",
    "        print(\"Select a dataframe to rename columns in:\")\n",
    "        for i, name in enumerate(df_dict.keys()):\n",
    "            print(f\"{i+1}. {name}\")\n",
    "        df_num = input(\"Enter the dataframe index (or leave empty to stop): \")\n",
    "        if not df_num:\n",
    "            break\n",
    "        try:\n",
    "            df_num = int(df_num)\n",
    "            if df_num < 1 or df_num > len(df_dict):\n",
    "                raise ValueError\n",
    "        except ValueError:\n",
    "            print(\"Invalid index\")\n",
    "            continue\n",
    "        df_name = list(df_dict.keys())[df_num - 1]\n",
    "        column = input(\"Enter the column name: \")\n",
    "        new_name = input(\"Enter the new column name: \")\n",
    "        print(f\"Renaming column {column} to {new_name} in dataframe {df_name}...\")\n",
    "        df_dict[df_name].rename(column, new_name)\n",
    "        print(f\"Column renamed successfully\")\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def merge_dfs(df_dict):\n",
    "    while True:\n",
    "        print(\"Select dataframes to merge:\")\n",
    "        for i, name in enumerate(df_dict.keys()):\n",
    "            print(f\"{i+1}. {name}\")\n",
    "        df1_num = input(\"Enter the first dataframe index (or leave empty to stop): \")\n",
    "        if not df1_num:\n",
    "            break\n",
    "        try:\n",
    "            df1_num = int(df1_num)\n",
    "            if df1_num < 1 or df1_num > len(df_dict):\n",
    "                raise ValueError\n",
    "        except ValueError:\n",
    "            print(\"Invalid index\")\n",
    "            continue\n",
    "        df1_name = list(df_dict.keys())[df1_num - 1]\n",
    "        df2_num = input(\"Enter the second dataframe index: \")\n",
    "        try:\n",
    "            df2_num = int(df2_num)\n",
    "            if df2_num < 1 or df2_num > len(df_dict):\n",
    "                raise ValueError\n",
    "        except ValueError:\n",
    "            print(\"Invalid index\")\n",
    "            continue\n",
    "        df2_name = list(df_dict.keys())[df2_num - 1]\n",
    "        if df1_name == df2_name:\n",
    "            print(\"Dataframes must be different\")\n",
    "            continue\n",
    "        on = input(\"Enter the column to merge on: \")\n",
    "        how = input(\"Enter the merge method (inner, left, right, outer): \")\n",
    "        print(f\"Merging dataframes {df1_name} and {df2_name}...\")\n",
    "        df_dict[df1_name].merge(df_dict[df2_name], on, how)\n",
    "        print(f\"Dataframes merged successfully\")\n",
    "        df_dict.pop(df2_name)\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def export_dfs(df_dict):\n",
    "    for name, df in df_dict.items():\n",
    "        json_filename = input(f\"Enter the json filename for dataframe {name}: \")\n",
    "        json_filepath = OUT_DIR + json_filename\n",
    "        print(f\"Exporting JSON file {json_filename} for dataframe {name}...\")\n",
    "        df.to_json(json_filepath)\n",
    "        print(f\"JSON file {json_filename} exported successfully\")\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    df_dict = load_dfs()\n",
    "    if not df_dict:\n",
    "        print(\"Exited due to error\")\n",
    "        return\n",
    "    print()\n",
    "    df_dict = clean_dfs(df_dict)\n",
    "    print()\n",
    "    df_dict = remove_rows(df_dict)\n",
    "    print()\n",
    "    df_dict = rename_columns(df_dict)\n",
    "    print()\n",
    "    df_dict = merge_dfs(df_dict)\n",
    "    print()\n",
    "    export_dfs(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV file activity_log.csv...\n",
      "CSV file activity_log.csv loaded to dataframe al successfully\n",
      "Loading CSV file component_codes.csv...\n",
      "CSV file component_codes.csv loaded to dataframe cc successfully\n",
      "Loading CSV file user_log.csv...\n",
      "CSV file user_log.csv loaded to dataframe ul successfully\n",
      "\n",
      "Cleaning dataframe al...\n",
      "Dataframe al cleaned successfully\n",
      "Cleaning dataframe cc...\n",
      "Dataframe cc cleaned successfully\n",
      "Cleaning dataframe ul...\n",
      "Dataframe ul cleaned successfully\n",
      "\n",
      "Select a dataframe to remove rows from:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "Invalid index\n",
      "Select a dataframe to remove rows from:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "Removing rows where Component is System from dataframe al...\n",
      "Rows removed successfully\n",
      "Select a dataframe to remove rows from:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "Removing rows where Component is Folder from dataframe al...\n",
      "Rows removed successfully\n",
      "Select a dataframe to remove rows from:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "Removing rows where Component is System from dataframe cc...\n",
      "Rows removed successfully\n",
      "Select a dataframe to remove rows from:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "Removing rows where Component is Folder from dataframe cc...\n",
      "Rows removed successfully\n",
      "Select a dataframe to remove rows from:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "\n",
      "Select a dataframe to rename columns in:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "Renaming column User Full Name *Anonymized to User_ID in dataframe al...\n",
      "Column renamed successfully\n",
      "Select a dataframe to rename columns in:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "Renaming column User Full Name *Anonymized to User_ID in dataframe ul...\n",
      "Column renamed successfully\n",
      "Select a dataframe to rename columns in:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "\n",
      "Select dataframes to merge:\n",
      "1. al\n",
      "2. cc\n",
      "3. ul\n",
      "Merging dataframes ul and al...\n",
      "Dataframes merged successfully\n",
      "Select dataframes to merge:\n",
      "1. cc\n",
      "2. ul\n",
      "Merging dataframes ul and cc...\n",
      "Dataframes merged successfully\n",
      "Select dataframes to merge:\n",
      "1. ul\n",
      "\n",
      "Exporting JSON file ul.json for dataframe ul...\n",
      "JSON file ul.json exported successfully\n"
     ]
    }
   ],
   "source": [
    "prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
