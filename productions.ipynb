{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production 1: Data translation for storage\n",
    "\n",
    "To enable a program to function each time it runs there needs to be an external and persistent data storage system that retains the state of the data. There are two considerations here. The physical storage medium for the data, such as a file or database and the format/struture of the data. This week you have explored a number of different formats in both regards as follows:\n",
    "\n",
    "- CSV\n",
    "- XML\n",
    "- JSON\n",
    "- MongoDB\n",
    "- SQL Database\n",
    "\n",
    "Select ONE format that you consider as most suited to the data in the scenario and the aims of the program (client brief/or your own data). The format selected should support both the nature of the data and the aims of the application being designed. It should provide distinct advantages and minimal limitations over other data formats. It should not be selected solely because it is the easiest to program, although this can be included as an advantage if applicable.\n",
    "\n",
    "### Design\n",
    "\n",
    "Produce a model that shows how the data needs to be restructured to take best advantage of the selected format and work more effectively within the program. Where you have created groups or objects from the data show how they relate to each other.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Implement a parser that reads in the original data file. You may want to create a subset of the data file for testing and speed. Your program should then perform the translation from the original format/structure into your selected format. The result of this process should then be outputted to its relevant physical medium (files/database).\n",
    "\n",
    "At this stage there is no requirement to handle data types (other than those inherent in the data format, i.e. numbers and “Strings”), conversions or missing data. The program can be demonstrated as a simple console based application, requiring the input of the file name by the user and sufficient output to demonstrate the correctness of the translation process.\n",
    "\n",
    "Your program should produce regular output statements to the console so that it is easy to follow what the program is doing and provides a visual demonstration of the translation process. This will also eb handy for any debugging required.\n",
    "\n",
    "### Reflection on design decisions\n",
    "\n",
    "Write a 200-word reflection that states the reason for your format selection and the advantages the format leads to the data and application and any limitations on the future use of this data within the selected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = 'datasets/'\n",
    "OUT_DIR = 'outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV parser\n",
    "\n",
    "\n",
    "class CSV:\n",
    "    def __init__(self, csv_file):\n",
    "        self.__file = csv_file\n",
    "        self.__headers = []\n",
    "        self.__data = []\n",
    "        self.__parse()\n",
    "\n",
    "    def __readln(self, line):\n",
    "        return line.strip().split(\",\")\n",
    "\n",
    "    def __parse(self):\n",
    "        with open(self.__file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if not self.__headers:\n",
    "                    self.__headers = self.__readln(line)\n",
    "                else:\n",
    "                    self.__data.append(self.__readln(line))\n",
    "\n",
    "    @property\n",
    "    def headers(self):\n",
    "        return self.__headers\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.__data\n",
    "\n",
    "    def to_dict(self):\n",
    "        return [\n",
    "            {self.__headers[i]: row[i] for i in range(len(self.__headers))}\n",
    "            for row in self.__data\n",
    "        ]\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main functions\n",
    "\n",
    "\n",
    "def parse_csv():\n",
    "    csv_filename = input(\"Enter the csv filename: \")\n",
    "    csv_filepath = SRC_DIR + csv_filename\n",
    "    if not os.path.exists(csv_filepath):\n",
    "        print(f\"File {csv_filepath} not found\")\n",
    "        return None\n",
    "    print(f\"Parsing CSV file {csv_filename}...\")\n",
    "    csv_parser = CSV(csv_filepath)\n",
    "    print(f\"CSV file {csv_filename} parsed successfully\")\n",
    "    return csv_parser\n",
    "\n",
    "\n",
    "def export_json(csv_parser):\n",
    "    json_filename = input(\"Enter the json filename: \")\n",
    "    json_filepath = OUT_DIR + json_filename\n",
    "    print(f\"Exporting JSON file {json_filename}...\")\n",
    "    os.makedirs(os.path.dirname(json_filepath), exist_ok=True)\n",
    "    with open(json_filepath, \"w\") as f:\n",
    "        f.write(csv_parser.to_json())\n",
    "    print(f\"JSON file {json_filename} exported successfully\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_parser = parse_csv()\n",
    "    if csv_parser:\n",
    "        print()\n",
    "        export_json(csv_parser)\n",
    "    else:\n",
    "        print(\"Exited due to error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production 3: Data Cleaning and Initial Analysis\n",
    "\n",
    "Given the client brief, there are a number of requirements to provide accesses to specific parts of the data and provide answers to specific statistical questions. For this production focus on how your application will manipulate the data (cleaning and shaping) and developing functions for calculating some of the statically requirements.\n",
    "\n",
    "### Design\n",
    "\n",
    "Consider the steps required for cleaning and shaping and any of the calculations (functions) you want to develop. Write pseudocode to sketch these out before you write code. Mentally or on paper walk the data through your pseudocode steps to test how effective your solution is.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The first stage is to clean the data and make sure it is fit for purpose. Examine the data careful to identify anomalies and then consider how your program can identify these and correct/delete or change. You will need to consider how you are going to handle erroneous or missing values. You should output a sample of the data that demonstrates how cleaning has changed the data.\n",
    "\n",
    "The next stage is to reshape the data as per any requirements of the brief (or your own scenario). Is all the data needed to provide the required results? Is any of it duplicated? Is there data across different sources that needs to be brought together? Again, output a sample to the console to demonstrate how this has changed the structure of the data.\n",
    "\n",
    "Finally develop and test a set of functions (or objects and methods) that applies the statistical analysis to the data set, outputting the results to the console.\n",
    "\n",
    "Capture the results of your data cleaning, shaping and functions with screenshots of the consol. There is no requirement at this stage for anything to be functioning in through the GUI. Make sure it is clear what your output is testing/demonstrating (output simple informative statements).\n",
    "\n",
    "### Reflection on design decisions\n",
    "\n",
    "Write a 200-word reflection that states why you have selected specific tools from NumPy or pandas. This may be the data structure you have used, the functions you have applied to clean and shape the data. Clearly identify which specific aspects of the requirements or data’s structure informed the decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF:\n",
    "    def __init__(self, csv_file):\n",
    "        self.__df = pd.read_csv(csv_file)\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self.__df.columns\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.__df\n",
    "\n",
    "    def remove(self, column: str, value: object) -> None:\n",
    "        \"\"\"Remove rows where column is equal to value\"\"\"\n",
    "        self.__df = self.__df[self.__df[column] != value]\n",
    "\n",
    "    def rename(self, column: str, new_name: str) -> None:\n",
    "        \"\"\"Rename a column\"\"\"\n",
    "        self.__df.rename(columns={column: new_name}, inplace=True)\n",
    "\n",
    "    def merge(self, df: \"DF\", on: str, how: str = 'inner') -> None:\n",
    "        \"\"\"Merge two dataframes\"\"\"\n",
    "        self.__df = self.__df.merge(df.data, on=on, how=how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
